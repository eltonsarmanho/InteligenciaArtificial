{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDES NEURAIS\n",
    "\n",
    "Este notebook aborda os algoritmos de redes neurais do capítulo 18 do livro *Inteligência Artificial: Uma Abordagem Moderna*, de Stuart Russel e Peter Norvig. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Motivação Biológica, o Perceptron e Funções de Ativação Típicas\n",
    "\n",
    "A origem das *redes neurais artificiais* (ANNs) remonta aos anos 1940. A forma mais simples de uma ANN é o *perceptron*, desenvolvido por Frank Rosenblatt em 1957 (os leitores interessados podem acessar o relatório original [aqui](https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf)), sendo este inspirado na biologia (veja o esboço simplificado de um neurônio biológico abaixo).\n",
    "\n",
    "Em um perceptron, a saída \\( y \\) é calculada ao tomar a combinação linear das entradas com os pesos \\( w_1, w_2 \\) e o *bias* \\( b \\), resultando no valor \\( z \\) (veja a parte direita da figura abaixo), ao qual uma função não linear \\( f \\) (a *função de ativação*) é aplicada ao final. \n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./assets/nn_regression/biological_neuron_analogy.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primeiro exemplo, consideremos a classificação binária, ou seja, duas classes (representadas por círculos e cruzes na parte direita da figura abaixo), que tentamos distinguir usando duas características de entrada $ x_1, x_2 $. Este cenário pode ser, por exemplo, um problema médico, onde queremos prever se um paciente possui uma determinada doença com base em dois fatores (por exemplo, idade e altura). Uma forma muito simples de função de ativação é a função de ativação Heaviside, para a qual o perceptron gera a seguinte saída:\n",
    "\n",
    "$ y= f(z)=\n",
    "  \\begin{cases}\n",
    "   1 \\quad \\text{if } z = w_1x_1+w_2x_2+b > 0 \\\\\n",
    "   0 \\quad \\text{otherwise.}\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "Aqui, os dois possíveis valores $ 0 $ e $ 1 $ representam as duas classes distintas. Dados pesos fixos $ w_1, w_2 $, o termo de *bias* $ b $ define quando o neurônio dispara (isto é, $ f(z) = 1 $) ou não, para as características de entrada $ x_1, x_2 $, que representam, por exemplo, um paciente.\n",
    "\n",
    "<img src=\"./assets/nn_regression/perceptron_binary_classification.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em geral, no aprendizado supervisionado, temos um conjunto de $ m $ exemplos de treinamento, que podem ser escritos como tuplas consistindo em características de entrada (neste exemplo resumidas em um vetor bidimensional $ \\mathbf{x} $, com componentes $ x_1 $ e $ x_2 $) e os rótulos corretos $ y^{(i)} $, com $ i = 1, ..., m $:\n",
    "\n",
    "$(\\mathbf{x}^{(1)}, y^{(1)}), (\\mathbf{x}^{(2)}, y^{(2)}), ..., (\\mathbf{x}^{(m)}, y^{(m)})$.\n",
    "\n",
    "\n",
    "Os pesos e o termo de *bias* do modelo (neste caso: $ w_1, w_2, b $) são otimizados minimizando uma *função de perda* $ L(w_1, w_2, b) $, que quantifica como os valores previstos $ \\hat{y}^{(i)} $ se desviam dos valores reais $ y^{(i)} $ - como uma função dos parâmetros do modelo. Normalmente, o gradiente descendente é usado para encontrar os parâmetros que minimizam $ L $ (com modificações no gradiente descendente permitindo, por exemplo, uma convergência mais rápida - veja os capítulos 5 e 8 desta [referência](https://www.deeplearningbook.org/)). Após finalizar a otimização, no caso de classificação, o desempenho do modelo pode ser avaliado por meio da precisão de classificação (número de previsões corretas dividido pelo número total de previsões).\n",
    "\n",
    "Neste exemplo, pode-se pensar na fase de treinamento / otimização como a modificação dos parâmetros do modelo de forma que a posição ideal de uma linha reta (veja a figura acima, à direita) seja encontrada, servindo como uma fronteira de decisão entre as duas classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para concluir esta seção sobre perceptrons, observe que a função de ativação Heaviside não é utilizada em aplicações modernas de aprendizado profundo. Em vez disso, são utilizadas as seguintes funções:\n",
    "\n",
    "- **Sigmoide:**  \n",
    "$\n",
    "f(x) = \\frac{1}{1 + \\exp{(-x)}}\n",
    "$\n",
    "\n",
    "- **Tangente hiperbólica:**  \n",
    "$\n",
    "f(x) = \\tanh{(x)} = \\frac{\\sinh{(x)}}{\\cosh{(x)}} = \\frac{\\exp{(x)} - \\exp{(-x)}}{\\exp{(x)} + \\exp{(-x)}}\n",
    "$\n",
    "\n",
    "- **Unidade linear retificada (ReLU):**  \n",
    "$\n",
    "f(x) = \\max(0, x)\n",
    "$\n",
    "\n",
    "A função de ativação ReLU é a mais frequentemente utilizada. Vale ressaltar que o uso de funções não lineares é essencial: se nenhuma função de ativação fosse utilizada, ou seja, apenas a identidade - também chamada de *função de ativação linear* - a classe de funções que o modelo poderia representar seria drasticamente reduzida.\n",
    "\n",
    "![activation_functions.png](./assets/nn_regression/activation_functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Perceptron Multicamadas (Multilayer peceptron)\n",
    "\n",
    "Expandindo a ideia de perceptrons simples, é possível construir perceptrons multicamadas como uma sequência de camadas.  \n",
    "Cada camada consiste em um número pré-definido de neurônios, onde os neurônios da primeira camada (a *camada de entrada*) correspondem às características de entrada $\\mathbf{x} = (x_1, x_2, ...) $. As camadas subsequentes são chamadas de *camadas ocultas*. Os neurônios individuais em cada camada oculta são uma combinação linear dos neurônios da camada anterior. Por exemplo, o *valor de ativação* $a_1 $ destacado na figura abaixo é calculado da seguinte forma:\n",
    "\n",
    "$\n",
    "a_1 = f(w_1 x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5 + w_6 x_6 + b_1),\n",
    "$\n",
    "\n",
    "onde uma função de ativação $f $ é aplicada para gerar o valor de ativação final. Esse processo, chamado de *propagação para frente* (*forward propagation*), é repetido para todos os neurônios e camadas até que a saída $\\textbf{o} = (o_1, o_2, ...) $ na *camada de saída* seja obtida. Já nesse nível, uma das principais características das redes neurais (multicamadas) se torna evidente — a capacidade de aprender recursos complexos a partir da entrada inicial. Essa habilidade de aprender novas representações que se tornam mais abstratas conforme a profundidade da rede (isto é, quanto mais camadas ocultas ela possui) distingue as redes neurais de outros algoritmos de aprendizado de máquina \"padrão\" (por exemplo, árvores de decisão). \n",
    "\n",
    "Vale observar que a arquitetura mostrada na figura abaixo possui apenas uma camada oculta (uma *rede neural rasa*), enquanto modelos modernos de aprendizado profundo podem alcançar mais de 50 camadas (veja, por exemplo, [aqui](http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)).\n",
    "\n",
    "<img src=\"./assets/nn_regression/mlp_example.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De forma mais formal, dada uma entrada vetorial $\\mathbf{x} = (x_1, x_2, ...)^T $, as ativações $\\mathbf{a} = (a_1, a_2, ...)^T $ da próxima camada são obtidas pela transformação:\n",
    "\n",
    "$\n",
    "\\mathbf{a} = f ( A\\mathbf{x} + \\mathbf{b} ),\n",
    "$\n",
    "\n",
    "que é essencialmente uma transformação afim (definida por uma matriz $A $ e um vetor $\\mathbf{b} $) seguida pela aplicação elemento a elemento da função de ativação (não linear) $f $. Os pesos das combinações lineares são organizados na matriz $A $ e os deslocamentos no *vetor de bias* $\\mathbf{b} = (b_1, b_2, ...) $. As ativações de saída $\\mathbf{o} $ são obtidas aplicando uma outra transformação afim (matriz $A^\\prime $, bias $\\mathbf{b}^\\prime $) e uma função de ativação $f^\\prime $:\n",
    "\n",
    "$\n",
    "\\mathbf{o} = f^\\prime (A^\\prime \\mathbf{a} + \\mathbf{b}^\\prime).\n",
    "$\n",
    "\n",
    "A função de ativação final $f^\\prime $ é escolhida de acordo com a tarefa, geralmente dependendo se é um problema de regressão ou classificação — voltaremos a isso mais tarde.\n",
    "\n",
    "Para simplificar a expressão acima para $\\mathbf{o} $, é comum modificar a definição do vetor de entrada e das matrizes de peso para que os termos de bias possam ser omitidos. Para ilustrar isso, considere o caso simplificado de duas características de entrada e duas ativações $a_1 = w_{11}x_1 + w_{12}x_2 + b_1 $ e $a_2 = w_{21}x_1 + w_{22}x_2 + b_2 $. Assim, $A $ e $\\mathbf{b} $ são definidos como:\n",
    "\n",
    "$\n",
    "A = \\begin{bmatrix}w_{11} & w_{12}\\\\w_{21} & w_{22}\\end{bmatrix}, \\quad \\mathbf{b} = \\begin{bmatrix}b_1 \\\\ b_2 \\end{bmatrix}.\n",
    "$\n",
    "\n",
    "Introduzindo as novas definições:\n",
    "\n",
    "$\n",
    "W = \\begin{bmatrix} b_1 & w_{11} & w_{12}\\\\ b_2 & w_{21} & w_{22}\\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix},\n",
    "$\n",
    "\n",
    "permite omitir o termo de bias e substituir $A\\mathbf{x}+\\mathbf{b} $ por $W\\mathbf{x} $. Voltando ao exemplo anterior, introduzimos as matrizes de peso $W $ e $W^\\prime $, o que gera uma expressão mais compacta para a saída:\n",
    "\n",
    "$\n",
    "\\mathbf{o} = f^\\prime (W^\\prime \\mathbf{a}) = f^\\prime (W^\\prime f(W \\mathbf{x})).\n",
    "$\n",
    "\n",
    "Essa equação torna mais claro que um perceptron multicamadas é uma concatenação de transformações afins e não lineares, parametrizadas por um conjunto de pesos e bias que são otimizados para se ajustar à tarefa em questão. Essa equação pode ser generalizada para um número arbitrário de camadas $N $:\n",
    "\n",
    "$\n",
    "\\mathbf{o} = f^{N} ( W^N f^{N-1} ( W^{N-1} ... f^1 ( W^1 \\mathbf{x}) ).\n",
    "$\n",
    "\n",
    "Um exemplo para o caso $N = 3 $ é mostrado na figura a seguir:\n",
    "\n",
    "<img src=\"./assets/nn_regression/mlp_more_layers_example.png\" width=\"200\">\n",
    "\n",
    "Quanto maior e mais profunda a rede, mais custoso computacionalmente se torna calcular o gradiente da função de perda (necessário para otimizar os parâmetros via gradiente descendente). Uma invenção chave é o algoritmo de retropropagação ([publicação original](https://www.nature.com/articles/323533a0)), que é uma forma eficiente de calcular o gradiente da função de perda em relação aos parâmetros da rede neural.\n",
    "\n",
    "Observe que, se usássemos apenas funções de ativação lineares (isto é, identidade), a saída seria essencialmente apenas uma combinação linear da entrada. O uso de funções de ativação não lineares (hoje em dia, principalmente a ReLU) enriquece o espaço de funções que podem ser representadas. Em particular, o teorema da aproximação universal (veja [capítulo 6.4.1](https://www.deeplearningbook.org/) e referências nele) garante que perceptrons multicamadas podem aproximar qualquer função contínua — embora este teorema não forneça detalhes sobre a arquitetura ou garantias de generalização. Devido ao uso de não linearidades, otimizar uma rede neural profunda é um problema de otimização não convexa com múltiplos mínimos locais. Técnicas como o gradiente descendente estocástico (veja [capítulo 8](https://www.deeplearningbook.org/)) ajudam a evitar ficar preso em mínimos locais não ótimos.\n",
    "\n",
    "Por fim, quanto à escolha da função de ativação na última camada, a função de ativação softmax é a escolha usual para tarefas de classificação, resultando na seguinte expressão para o $j $-ésimo componente do vetor de saída:\n",
    "\n",
    "$\n",
    "[\\mathbf{o}]_j = [f(\\mathbf{x})]_j = \\frac{\\exp{\\left([\\mathbf{x}]_j\\right)}}{\\sum_k \\exp{\\left([\\mathbf{x}]_k\\right)}}.\n",
    "$\n",
    "\n",
    "Dessa forma, os componentes do vetor de saída são normalizados para o intervalo $[0, 1] $ e somam um, ou seja, os componentes podem ser interpretados como probabilidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ilustrar a utilidade das funções de ativação softmax, considere o caso de classificação de estruturas cristalinas. A tarefa consiste em atribuir o rótulo correto de simetria a uma estrutura cristalina desconhecida, definida por posições atômicas e espécies químicas. Por exemplo, atribuições possíveis poderiam ser: cúbico de face centrada (*face-centered cubic* - fcc), cúbico de corpo centrado (*body-centered cubic* - bcc), diamante ou hexagonal compactado (*hexagonal close-packed* - hcp) — uma coleção de estruturas que cobre mais de 80% dos sólidos elementares. Explicações mais detalhadas sobre o aprendizado profundo aplicado à classificação de estruturas cristalinas podem ser encontradas [aqui](https://www.nature.com/articles/s41467-018-05169-6).\n",
    "\n",
    "Ao aplicar a arquitetura de perceptron multicamadas que introduzimos acima, cada um dos quatro neurônios de saída corresponde a uma estrutura cristalina específica. O uso da função de ativação softmax garante que todas as ativações de saída somem um, razão pela qual o vetor de saída $\\mathbf{o} $ pode ser considerado como um vetor de probabilidades de classificação. Por exemplo, se $\\mathbf{o} = (1, 0, 0, 0) $, a estrutura de entrada é prevista como tendo simetria fcc com 100% de probabilidade (veja a figura abaixo). Esse processo é também chamado de *one-hot encoding* e corresponde a representar um número $N $ de classes na base padrão de $\\mathbb{R}^N $, ou seja, por $N $ vetores $e_i = (0, ..., 0, 1, 0, ..., 0) $, para $i = 1, ..., N $, com todos os componentes de $e_i $ sendo zero, exceto o $i $-ésimo elemento.\n",
    "\n",
    "\n",
    "<img src=\"./assets/nn_regression/cs_classification_first_example.png\" width=\"1700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para regressão, a arquitetura de rede multicamada apresentada acima pode ser facilmente adaptada — utilizando apenas um neurônio na camada de saída juntamente com uma função de ativação *linear* (isto é, a função identidade) para prever uma propriedade-alvo específica $\\text{P} \\in \\mathbb{R} $. Uma arquitetura semelhante (com camadas ocultas adicionais) será utilizada neste tutorial, e a propriedade-alvo será preço dos imóveis de SP.\n",
    "\n",
    "<img src=\"./assets/nn_regression/regression_first_example.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BACKPROPAGATION\n",
    "\n",
    "### Visão Geral\n",
    "\n",
    "Tanto no Perceptron quanto na Rede Neural, utilizamos o algoritmo de Retropropagação (*Backpropagation*) para treinar nosso modelo atualizando os pesos. Isso é feito propagando os erros da última camada (camada de saída) de volta para a primeira camada (camada de entrada), razão pela qual o método é chamado de Retropropagação. Para utilizar a Retropropagação, precisamos de uma função de custo. Essa função é responsável por indicar o quão boa é a rede neural para um determinado exemplo. Uma função de custo comum é o *Erro Quadrático Médio* (*Mean Squared Error* - MSE). A função de custo tem o seguinte formato:\n",
    "\n",
    "$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y - \\hat{y})^{2}\n",
    "$\n",
    "\n",
    "Onde $n $ é o número de exemplos de treinamento, $\\hat{y} $ é a previsão feita pela rede e $y $ é o valor correto para o exemplo.\n",
    "\n",
    "O algoritmo combina os conceitos de derivadas parciais e a regra da cadeia para gerar o gradiente de cada peso na rede com base na função de custo.\n",
    "\n",
    "Por exemplo, se utilizarmos uma Rede Neural com três camadas, a função sigmoide como função de ativação e o MSE como função de custo, e quisermos encontrar o gradiente de um peso $w_{j} $, podemos calculá-lo assim:\n",
    "\n",
    "$\n",
    "\\frac{\\partial MSE(\\hat{y}, y)}{\\partial w_{j}} = \\frac{\\partial MSE(\\hat{y}, y)}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}(in_{j})}{\\partial in_{j}} \\times \\frac{\\partial in_{j}}{\\partial w_{j}}\n",
    "$\n",
    "\n",
    "Resolvendo esta equação, temos:\n",
    "\n",
    "$\n",
    "\\frac{\\partial MSE(\\hat{y}, y)}{\\partial w_{j}} = (\\hat{y} - y) \\times \\hat{y}'(in_{j}) \\times a_{j}\n",
    "$\n",
    "\n",
    "Lembre-se de que $\\hat{y} $ é a função de ativação aplicada a um neurônio em nossa camada oculta, portanto:\n",
    "\n",
    "$\n",
    "\\hat{y} = sigmoid\\left(\\sum_{i=1}^{num\\_neurons} w_{i} \\times a_{i}\\right)\n",
    "$\n",
    "\n",
    "Além disso, $a $ é a entrada gerada ao alimentar as variáveis da camada de entrada na camada oculta.\n",
    "\n",
    "Podemos usar a mesma técnica para calcular os gradientes dos pesos na camada de entrada. Após calcular os gradientes para os pesos, utilizamos o gradiente descendente para atualizar os pesos da rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK ALGORITHM\n",
    "\n",
    "### Visão Geral\n",
    "\n",
    "Embora o Perceptron possa parecer uma boa maneira de realizar classificações, ele é um classificador linear (o que, grosso modo, significa que só pode traçar linhas retas para dividir espaços) e, portanto, pode enfrentar dificuldades com problemas mais complexos. Para resolver esse problema, podemos estender o Perceptron empregando múltiplas camadas de sua funcionalidade. O resultado dessa construção é chamado de Rede Neural, ou Perceptron Multicamadas, e é um classificador não linear. Ele alcança isso combinando os resultados de funções lineares em cada camada da rede.\n",
    "\n",
    "Semelhante ao Perceptron, essa rede também possui uma camada de entrada e uma camada de saída; no entanto, ela também pode conter várias camadas ocultas. Essas camadas ocultas são responsáveis pela não linearidade da rede. As camadas são compostas por nós. Cada nó em uma camada (exceto na camada de entrada) contém alguns valores chamados *pesos* e recebe como entrada os valores de saída da camada anterior. O nó, então, calcula o produto escalar de suas entradas e seus pesos, ativando-o em seguida com uma *função de ativação* (por exemplo, função de ativação sigmoide). Sua saída é então enviada para os nós da próxima camada. Vale notar que, às vezes, a camada de saída não utiliza uma função de ativação ou utiliza uma diferente do restante da rede. O processo de passar as saídas para frente entre as camadas é chamado de *feed-forward*.\n",
    "\n",
    "Depois que os valores de entrada são passados adiante na rede, a saída resultante pode ser usada para classificação. O problema agora é como treinar a rede (isto é, ajustar os pesos nos nós). Para isso, utilizamos o algoritmo de *Retropropagação* (*Backpropagation*). Em resumo, ele faz o oposto do que estávamos fazendo até este ponto. Em vez de passar as entradas para frente, ele rastreia o erro de trás para frente. Assim, após realizarmos uma classificação, verificamos se está correta ou não e quão distante estávamos. Em seguida, pegamos esse erro e o propagamos para trás na rede, ajustando os pesos dos nós conforme necessário. O algoritmo será executado no conjunto de dados de entrada por um tempo fixo ou até que fiquemos satisfeitos com os resultados. O número de vezes que iteramos sobre o conjunto de dados é chamado de *épocas* (*epochs*). Em uma seção posterior, analisamos detalhadamente como esse algoritmo funciona.\n",
    "\n",
    "**NOTA:** Às vezes, adicionamos outro nó à entrada de cada camada, chamado de *bias*. Este é um valor constante que será enviado para a próxima camada, geralmente configurado como 1. O *bias* geralmente nos ajuda a \"deslocar\" a função calculada para a esquerda ou para a direita."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neural_net](images/neural_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementação\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar Bibliotecas Necessárias:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar e Pré-processar os Dados:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset\n",
    "df = pd.read_csv('./data/sao-paulo-properties-april-2019.csv')\n",
    "\n",
    "# Selecionar as colunas relevantes\n",
    "features = ['Condo', 'Size', 'Rooms', 'Toilets', 'Suites', 'Parking', 'Elevator', 'Furnished', 'Swimming Pool', 'New']\n",
    "target = 'Price'\n",
    "\n",
    "# Remover linhas com valores faltantes\n",
    "#df = df.dropna(subset=features + [target])\n",
    "# Separar features e target\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "# Converter variáveis categóricas em dummy/indicator variables\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir e Treinar o Modelo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o MLPRegressor\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=(120, 120),  # Duas camadas ocultas com 64 e 32 neurônios\n",
    "    activation='relu',            # Função de ativação ReLU\n",
    "    solver='adam',                # Otimizador Adam\n",
    "    max_iter=1000,                 # Número máximo de iterações\n",
    "    random_state=42,\n",
    "    learning_rate = 'adaptive'\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliar o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de avaliação\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotar valores reais vs. previsões\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Valores Reais')\n",
    "plt.ylabel('Previsões')\n",
    "plt.title('Valores Reais vs. Previsões')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural Multicamada usando Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar Bibliotecas Necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregar e Pré-processar os Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset\n",
    "df = pd.read_csv('./data/sao-paulo-properties-april-2019.csv')\n",
    "\n",
    "# Selecionar as colunas relevantes\n",
    "features = ['Condo', 'Size', 'Rooms', 'Toilets', 'Suites', 'Parking', 'Elevator', 'Furnished', 'Swimming Pool', 'New']\n",
    "target = 'Price'\n",
    "\n",
    "# Remover linhas com valores faltantes\n",
    "#df = df.dropna(subset=features + [target])\n",
    "# Separar features e target\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "# Converter variáveis categóricas em dummy/indicator variables\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir a Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar a rede neural\n",
    "model = Sequential()\n",
    "\n",
    "# Adicionar a primeira camada oculta\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# Adicionar a segunda camada oculta\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "\n",
    "# Adicionar a camada de saída\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinar a Rede Neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliar o Modelo\n",
    "\n",
    "#### Explicação das Métricas utilizadas para regressão:\n",
    "\n",
    "1. **Mean Squared Error (MSE):**\n",
    "   - Mede o erro médio quadrático entre os valores reais e as previsões.\n",
    "   - Quanto menor o MSE, melhor o modelo.\n",
    "   - Fórmula:\n",
    "     $\n",
    "     \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "     $\n",
    "\n",
    "2. **Mean Absolute Error (MAE):**\n",
    "   - Mede o erro médio absoluto entre os valores reais e as previsões.\n",
    "   - É menos sensível a outliers do que o MSE.\n",
    "   - Fórmula:\n",
    "     $\n",
    "     \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "     $\n",
    "\n",
    "3. **R² Score (Coeficiente de Determinação):**\n",
    "   - Mede o quanto o modelo explica a variância dos dados.\n",
    "   - Varia de 0 a 1. Quanto mais próximo de 1, melhor o modelo.\n",
    "   - Fórmula:\n",
    "     $\n",
    "     R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "     $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o modelo no conjunto de teste\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Loss no conjunto de teste: {loss}')\n",
    "\n",
    "# Fazer previsões\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular métricas de avaliação\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error (MSE): {mse}')\n",
    "print(f'Mean Absolute Error (MAE): {mae}')\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotar a perda durante o treinamento\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neurais para Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 1. Carregar o dataset\n",
    "df = pd.read_csv(\"./aima-data/zoo.csv\", sep=',')\n",
    "\n",
    "# 2. Pré-processamento dos dados\n",
    "# Separar características e rótulo\n",
    "X = df.drop(['animal_name', 'type'], axis=1)\n",
    "y = df['type']\n",
    "\n",
    "# Converter rótulos para valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "y_categorical = to_categorical(y_encoded)\n",
    "\n",
    "# Normalizar as características\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. Dividir o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. Construir a rede neural\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(y_categorical.shape[1], activation='softmax'))\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 5. Treinar a rede neural\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.2)\n",
    "\n",
    "# 6. Avaliar o modelo\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Preparar os dados de entrada\n",
    "# Exemplo: Características de um animal (sem o nome e o tipo)\n",
    "# Vamos usar um exemplo de um mamífero (por exemplo, um lobo)\n",
    "# [hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic, catsize]\n",
    "input_data = np.array([[1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 4, 1, 0, 1]])  # Características de um lobo\n",
    "\n",
    "# Normalizar os dados de entrada (usando o mesmo scaler que foi usado no treino)\n",
    "input_data_scaled = scaler.transform(input_data)\n",
    "\n",
    "# 2. Fazer a predição\n",
    "prediction = model.predict(input_data_scaled)\n",
    "print(prediction)\n",
    "# 3. Interpretar o resultado\n",
    "# A saída é uma matriz de probabilidades para cada classe\n",
    "predicted_class_index = np.argmax(prediction, axis=1)  # Encontra o índice da classe com maior probabilidade\n",
    "predicted_class = label_encoder.inverse_transform(predicted_class_index)  # Converte o índice de volta para o rótulo original\n",
    "\n",
    "print(f\"Predicted class: {predicted_class[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROBABILIDADE POR CLASSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Preparar os dados de entrada\n",
    "# Exemplo: Características de um animal (sem o nome e o tipo)\n",
    "# Vamos usar um exemplo de um mamífero (por exemplo, um lobo)\n",
    "# [hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic, catsize]\n",
    "input_data = np.array([[1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 4, 1, 0, 1]])  # Características de um lobo\n",
    "\n",
    "# Normalizar os dados de entrada (usando o mesmo scaler que foi usado no treino)\n",
    "input_data_scaled = scaler.transform(input_data)\n",
    "\n",
    "# 2. Fazer a predição\n",
    "prediction = model.predict(input_data_scaled)\n",
    "\n",
    "# 3. Exibir as probabilidades de cada classe\n",
    "# A saída é uma matriz de probabilidades para cada classe\n",
    "print(\"Probabilidades de cada classe:\")\n",
    "for i, prob in enumerate(prediction[0]):\n",
    "    class_name = label_encoder.inverse_transform([i])[0]  # Converte o índice de volta para o nome da classe\n",
    "    print(f\"{class_name}: {prob:.4f}\")\n",
    "\n",
    "# 4. Interpretar o resultado\n",
    "predicted_class_index = np.argmax(prediction, axis=1)  # Encontra o índice da classe com maior probabilidade\n",
    "predicted_class = label_encoder.inverse_transform(predicted_class_index)  # Converte o índice de volta para o rótulo original\n",
    "\n",
    "print(f\"\\nPredicted class: {predicted_class[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
