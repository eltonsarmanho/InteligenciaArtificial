{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Demonstra√ß√£o Avan√ßada de Agentes com Gymnasium (Padr√£o OpenAI)\n",
    "\n",
    "Este notebook atende √† solicita√ß√£o de usar uma biblioteca de agentes mais sofisticada e f√°cil de usar, padr√£o na ind√∫stria de IA. Usaremos a **Gymnasium**, a sucessora da famosa biblioteca `Gym` da OpenAI.\n",
    "\n",
    "**O que faremos de diferente:**\n",
    "1.  **Criaremos nosso pr√≥prio Ambiente:** Modelaremos o \"Mundo do Aspirador de P√≥\" como um ambiente customizado na Gymnasium. Isso √© fundamental para aplicar IA a problemas novos.\n",
    "2.  **Implementaremos os Agentes:** Mostraremos como cada tipo de agente (Simples de Reflexo, Baseado em Modelo e de Aprendizado) interage com este ambiente padronizado.\n",
    "\n",
    "Isso conecta a teoria dos slides diretamente com a pr√°tica moderna de Aprendizado por Refor√ßo (RL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 1. Prepara√ß√£o: Instala√ß√£o das Bibliotecas\n",
    "\n",
    "Precisamos da `gymnasium` para criar o ambiente e da `numpy` para nos ajudar com a tabela do agente de aprendizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gymnasium numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß± 2. Construindo o Ambiente do Aspirador de P√≥\n",
    "\n",
    "Para que um agente possa interagir, primeiro precisamos definir as 'regras do jogo'. Um ambiente na Gymnasium precisa ter:\n",
    "\n",
    "- **Espa√ßo de A√ß√µes (`action_space`):** Quais a√ß√µes o agente pode tomar?\n",
    "  - `0`: Ir para a Esquerda\n",
    "  - `1`: Ir para a Direita\n",
    "  - `2`: Aspirar\n",
    "- **Espa√ßo de Observa√ß√µes (`observation_space`):** O que o agente pode 'ver' do ambiente?\n",
    "  - Vamos definir os estados poss√≠veis: `(Localiza√ß√£o, Status de A, Status de B)`\n",
    "- **Fun√ß√£o `reset()`:** Reinicia o ambiente para um estado inicial.\n",
    "- **Fun√ß√£o `step(action)`:** Executa uma a√ß√£o, atualiza o ambiente e retorna a nova observa√ß√£o, a recompensa, e se o epis√≥dio terminou."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class VacuumCleanerEnv(gym.Env):\n",
    "    \"\"\"Ambiente customizado do Aspirador de P√≥ seguindo a interface da Gymnasium.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VacuumCleanerEnv, self).__init__()\n",
    "        \n",
    "        # Definir o espa√ßo de a√ß√µes: 0: Esquerda, 1: Direita, 2: Aspirar\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        \n",
    "        # Definir o espa√ßo de observa√ß√£o: [localiza√ß√£o (0=A, 1=B), status_A (0=Limpo, 1=Sujo), status_B]\n",
    "        self.observation_space = spaces.MultiDiscrete([2, 2, 2])\n",
    "        \n",
    "        self.state = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # Estado inicial aleat√≥rio para melhor treinamento\n",
    "        self.state = self.observation_space.sample()\n",
    "        return np.array(self.state), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        location, status_A, status_B = self.state\n",
    "        reward = -1  # Custo de -1 para cada a√ß√£o (incentiva a efici√™ncia)\n",
    "        \n",
    "        if action == 0: # Ir para a Esquerda\n",
    "            self.state[0] = 0 # Vai para A\n",
    "        elif action == 1: # Ir para a Direita\n",
    "            self.state[0] = 1 # Vai para B\n",
    "        elif action == 2: # Aspirar\n",
    "            if location == 0 and status_A == 1: # Se est√° em A e A est√° sujo\n",
    "                self.state[1] = 0 # Limpa A\n",
    "                reward = 10 # Recompensa grande por limpar\n",
    "            elif location == 1 and status_B == 1: # Se est√° em B e B est√° sujo\n",
    "                self.state[2] = 0 # Limpa B\n",
    "                reward = 10 # Recompensa grande por limpar\n",
    "            else:\n",
    "                reward = -5 # Penalidade por tentar aspirar um local limpo\n",
    "        \n",
    "        # O epis√≥dio termina quando ambos os locais est√£o limpos\n",
    "        terminated = (self.state[1] == 0 and self.state[2] == 0)\n",
    "        \n",
    "        return np.array(self.state), reward, terminated, False, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(\"Estado atual:\",self.state)\n",
    "        location, status_A, status_B = self.state\n",
    "        sala_a = \"(A, Sujo)\" if status_A == 1 else \"(A, Limpo)\"\n",
    "        sala_b = \"(B, Sujo)\" if status_B == 1 else \"(B, Limpo)\"\n",
    "        \n",
    "        if location == 0:\n",
    "            print(f\"[*] {sala_a}  |  {sala_b}\")\n",
    "        else:\n",
    "            print(f\"  {sala_a}  |  [*] {sala_b}\")\n",
    "\n",
    "# Testando se o ambiente funciona\n",
    "env = VacuumCleanerEnv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 3. Agente Simples de Reflexo\n",
    "\n",
    "Este agente √© uma pol√≠tica \"hard-coded\". Ele n√£o aprende e n√£o tem mem√≥ria. Ele simplesmente mapeia uma observa√ß√£o (`state`) para uma a√ß√£o, seguindo regras fixas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_reflex_policy(observation):\n",
    "    \"\"\"Define a pol√≠tica de um agente simples de reflexo.\"\"\"\n",
    "    location, status_A, status_B = observation\n",
    "    \n",
    "    # Se o local atual est√° sujo, aspire\n",
    "    if location == 0 and status_A == 1:\n",
    "        return 2 # Aspirar\n",
    "    if location == 1 and status_B == 1:\n",
    "        return 2 # Aspirar\n",
    "    \n",
    "    # Se o local atual est√° limpo, mova-se\n",
    "    if location == 0:\n",
    "        return 1 # Ir para a Direita\n",
    "    else:\n",
    "        return 0 # Ir para a Esquerda\n",
    "\n",
    "# Executando o agente simples\n",
    "print(\"Executando Agente Simples de Reflexo:\")\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "total_reward = 0\n",
    "\n",
    "while not terminated:\n",
    "    env.render()\n",
    "    action = simple_reflex_policy(observation)\n",
    "    observation, reward, terminated, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"A√ß√£o Tomada: {['Esquerda', 'Direita', 'Aspirar'][action]} | Recompensa: {reward}\")\n",
    "\n",
    "print(\"\\nTarefa conclu√≠da!\")\n",
    "env.render()\n",
    "print(f\"Recompensa Total: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù 4. Agente de Reflexo Baseado em Modelo\n",
    "\n",
    "Este agente √© um pouco mais inteligente. Ele constr√≥i um **modelo interno** do mundo para evitar a√ß√µes desnecess√°rias. Por exemplo, se ele j√° sabe que a sala A est√° limpa, ele n√£o precisa ir at√© l√° para verificar novamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedAgent:\n",
    "    def __init__(self):\n",
    "        # O modelo armazena o que o agente sabe sobre o mundo\n",
    "        # None = Desconhecido, 0 = Limpo, 1 = Sujo\n",
    "        self.model = {'A': None, 'B': None}\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        location, status_A, status_B = observation\n",
    "        \n",
    "        # Atualiza o modelo com a percep√ß√£o atual\n",
    "        self.model['A'] = status_A\n",
    "        self.model['B'] = status_B\n",
    "        \n",
    "        print(f\"Modelo interno do agente: {self.model}\")\n",
    "        \n",
    "        # Se o local atual est√° sujo, aspire\n",
    "        if location == 0 and status_A == 1:\n",
    "            return 2 # Aspirar\n",
    "        if location == 1 and status_B == 1:\n",
    "            return 2 # Aspirar\n",
    "            \n",
    "        # Se o local atual est√° limpo, verifique o modelo para decidir para onde ir\n",
    "        if location == 0: # Estou em A, que est√° limpo\n",
    "            if self.model['B'] != 0: # Se n√£o sei que B est√° limpo, vou para l√°\n",
    "                return 1 # Direita\n",
    "        else: # Estou em B, que est√° limpo\n",
    "            if self.model['A'] != 0: # Se n√£o sei que A est√° limpo, vou para l√°\n",
    "                return 0 # Esquerda\n",
    "        \n",
    "        # Se ambos os locais s√£o conhecidos como limpos, a tarefa terminou (o loop principal vai parar)\n",
    "        return random.choice([0,1]) # A√ß√£o padr√£o se n√£o h√° mais o que fazer\n",
    "\n",
    "# Executando o agente baseado em modelo\n",
    "print(\"Executando Agente Baseado em Modelo:\")\n",
    "agent = ModelBasedAgent()\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "\n",
    "while not terminated:\n",
    "    env.render()\n",
    "    action = agent.choose_action(observation)\n",
    "    observation, reward, terminated, _, _ = env.step(action)\n",
    "    print(f\"A√ß√£o Tomada: {['Esquerda', 'Direita', 'Aspirar'][action]}\")\n",
    "    \n",
    "print(\"\\nTarefa conclu√≠da!\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì 5. Agente Baseado em Aprendizado (Q-Learning)\n",
    "\n",
    "Este √© o agente mais avan√ßado. Ele n√£o tem nenhuma regra pr√©-definida. Ele **aprende** a melhor pol√≠tica de a√ß√µes por tentativa e erro, usando as recompensas do ambiente. Usaremos o algoritmo Q-Learning, conforme a f√≥rmula do slide 49. \n",
    "\n",
    "$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]$$\n",
    "\n",
    "O estado `s` aqui ser√° um n√∫mero √∫nico para cada uma das 8 combina√ß√µes de observa√ß√£o `(loc, stat_A, stat_B)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperpar√¢metros para o treinamento\n",
    "num_episodes = 10\n",
    "learning_rate = 0.1 # Alpha\n",
    "discount_factor = 0.99 # Gamma\n",
    "epsilon = 1.0 # Probabilidade de explora√ß√£o (come√ßa alta)\n",
    "min_epsilon = 0.05\n",
    "epsilon_decay_rate = 0.9995\n",
    "\n",
    "# A tabela Q ter√° uma linha para cada estado e uma coluna para cada a√ß√£o.\n",
    "# Como nosso espa√ßo de observa√ß√£o √© [2, 2, 2], temos 2*2*2 = 8 estados poss√≠veis.\n",
    "q_table = np.zeros((env.observation_space.nvec.prod(), env.action_space.n))\n",
    "\n",
    "def observation_to_state(obs):\n",
    "    \"\"\"Converte um array de observa√ß√£o [l, sA, sB] em um √∫nico inteiro.\"\"\"\n",
    "    return obs[0] * 4 + obs[1] * 2 + obs[2]\n",
    "\n",
    "print(\"Iniciando o treinamento do Agente Q-Learning...\")\n",
    "\n",
    "# Loop de treinamento\n",
    "for episode in range(num_episodes):\n",
    "    observation, info = env.reset()\n",
    "    state = observation_to_state(observation)\n",
    "    terminated = False\n",
    "\n",
    "    while not terminated:\n",
    "        # Explora√ß√£o vs. Explota√ß√£o (Epsilon-greedy)\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # A√ß√£o aleat√≥ria (explorar)\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Melhor a√ß√£o conhecida (explorar)\n",
    "        \n",
    "        new_observation, reward, terminated, _, _ = env.step(action)\n",
    "        new_state = observation_to_state(new_observation)\n",
    "        \n",
    "        # Atualiza√ß√£o da Tabela Q\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[new_state])\n",
    "        \n",
    "        new_value = (1 - learning_rate) * old_value + learning_rate * (reward + discount_factor * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "    # Reduzir epsilon para diminuir a explora√ß√£o ao longo do tempo\n",
    "    if epsilon > min_epsilon:\n",
    "        epsilon *= epsilon_decay_rate\n",
    "\n",
    "print(\"Treinamento Conclu√≠do!\")\n",
    "print(f\"Epsilon final: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando o Agente Treinado\n",
    "\n",
    "Agora, vamos desativar a explora√ß√£o (`epsilon = 0`) e ver o agente usar a pol√≠tica que aprendeu para resolver o problema de forma √≥tima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nExecutando o Agente Q-Learning treinado (modo de avalia√ß√£o):\")\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "total_reward = 0\n",
    "\n",
    "while not terminated:\n",
    "    env.render()\n",
    "    state = observation_to_state(observation)\n",
    "    action = np.argmax(q_table[state]) # Sempre escolhe a melhor a√ß√£o\n",
    "    observation, reward, terminated, _, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"A√ß√£o Tomada: {['Esquerda', 'Direita', 'Aspirar'][action]} | Recompensa: {reward}\")\n",
    "\n",
    "print(\"\\nTarefa conclu√≠da de forma √≥tima!\")\n",
    "env.render()\n",
    "print(f\"Recompensa Total: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclus√£o\n",
    "\n",
    "Neste notebook, usamos a biblioteca `Gymnasium`, padr√£o da ind√∫stria, para explorar os conceitos da aula:\n",
    "\n",
    "1.  **Agente Simples de Reflexo:** Uma pol√≠tica fixa e reativa, f√°cil de implementar, mas ineficiente.\n",
    "2.  **Agente Baseado em Modelo:** Melhora a efici√™ncia ao manter um estado interno, mas ainda depende de l√≥gica pr√©-programada.\n",
    "3.  **Agente de Aprendizado (Q-Learning):** O mais poderoso. N√£o precisa de regras; ele **aprende a pol√≠tica √≥tima** interagindo com o ambiente, tornando-se capaz de resolver problemas complexos de forma aut√¥noma.\n",
    "\n",
    "Ferramentas como a Gymnasium s√£o a base para treinar agentes em tarefas muito mais complexas, desde jogos como Xadrez e Go at√© controle de rob√¥s e ve√≠culos aut√¥nomos. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
