{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 1: Estrutura Básica do LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Passo 1. Instalação das Dependências**\n",
    "   Primeiro, certifique-se de que você tem o LangChain instalado. Você pode instalá-lo usando pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-community python-dotenv openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Passo 2: Configurar a Chave da API**\n",
    "\n",
    "1. **Obtenha sua chave da API:**\n",
    "\n",
    "   - **OpenAI**: Acesse o [site da OpenAI](https://platform.openai.com/account/api-keys) e gere sua chave da API. Certifique-se de copiá-la e armazená-la em local seguro, pois você não poderá visualizá-la novamente.\n",
    "   - **Gemini**: Acesse o [AI Studio da Google](https://aistudio.google.com/app/apikey), faça login com sua conta Google e siga as instruções para gerar a chave de acesso à API Gemini.\n",
    "   - **DeepSeek**: Vá até o [site oficial da DeepSeek](https://www.deepseek.com/), faça login e gere sua chave da API no painel de configurações.\n",
    "\n",
    "2. **Armazene sua chave em um arquivo `.env`:**\n",
    "\n",
    "   Crie ou edite um arquivo chamado `.env` no diretório do seu projeto e adicione as chaves obtidas, seguindo o formato abaixo:\n",
    "\n",
    "   ```plaintext\n",
    "   OPENAI_API_KEY=your_openai_api_key_here\n",
    "   ```\n",
    "\n",
    "3. **Carregue as chaves no seu código:**\n",
    "\n",
    "   Use uma biblioteca como `python-dotenv` para carregar as variáveis de ambiente no seu programa:\n",
    "\n",
    "   ```python\n",
    "   from dotenv import load_dotenv\n",
    "   import os\n",
    "\n",
    "   # Carregar as variáveis do arquivo .env\n",
    "   load_dotenv()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Passo 3: Chamando um LLM com `invoke`**\n",
    "\n",
    "\n",
    "O método `invoke` é usado para enviar uma entrada ao LLM e receber uma resposta completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Carregar as variáveis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obter as chaves das APIs\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# Certifique-se de que as chaves foram carregadas\n",
    "if not all([openai_api_key, gemini_api_key, deepseek_api_key]):\n",
    "    raise ValueError(\"Uma ou mais chaves de API não foram configuradas corretamente.\")\n",
    "\n",
    "# Inicializando um LLM \n",
    "llm = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Chamando o LLM com uma entrada\n",
    "resposta = llm.invoke(\"Explique o que é um LLM.\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Passo 3.1: Chamando um LLM com `invoke` com Gemini**\n",
    "\n",
    "\n",
    "O método `invoke` é usado para enviar uma entrada ao LLM e receber uma resposta completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Set cache to save results to memory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# Carregar as variáveis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obter as chaves das APIs\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "# Correctly initialize the ChatGoogleGenerativeAI model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=gemini_api_key, top_p=0.30)\n",
    "\n",
    "# Prepare the input messages\n",
    "input_messages = [\n",
    "    HumanMessage(content=\"Quem é o presidente do Brasil?\")\n",
    "]\n",
    "\n",
    "# Use the model\n",
    "response = llm.invoke(input_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Passo 3.2: Chamando um LLM com `invoke` com Deepseek**\n",
    "\n",
    "\n",
    "O método `invoke` é usado para enviar uma entrada ao LLM e receber uma resposta completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Carrega a chave do .env\n",
    "\n",
    "# Configuração do DeepSeek\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    model=\"deepseek-chat\",  # ou \"deepseek-reasoner\"\n",
    "    openai_api_base=\"https://api.deepseek.com/v1\",  # Endpoint da API\n",
    "     \n",
    ")\n",
    "\n",
    "# Exemplo de uso\n",
    "response = llm.invoke(\"Quem é o presidente do Brasil?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Passo 3.3: Chamando um LLM com `invoke` com Maritalk**\n",
    "\n",
    "\n",
    "O método `invoke` é usado para enviar uma entrada ao LLM e receber uma resposta completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Que ótimo! Aqui estão três sugestões de nomes para o seu cão:\n",
      "\n",
      "1. Max\n",
      "2. Bolt\n",
      "3. Duke\n",
      "\n",
      "Espero que um desses nomes combine perfeitamente com a personalidade do seu cão!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatMaritalk\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Carrega a chave do .env\n",
    "\n",
    "\n",
    "llm = ChatMaritalk(\n",
    "    model=\"sabia-3\",  # Available models: sabia-3\n",
    "    api_key=os.getenv(\"MARITALK_API_KEY\"),  # Insert your API key here\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"você é um assistente especializado em sugestão de nome de pets. Dado animal, você deve sugerir 3 nomes.\",\n",
    "        ),\n",
    "        (\"human\", \"Eu tenho um {animal}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = chat_prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"animal\": \"cão\"})\n",
    "print(response)  # should answer something like \"1. Max\\n2. Bella\\n3. Charlie\\n4. Rocky\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stream de Respostas com `stream`\n",
    "\n",
    "O método `stream` permite receber a resposta do LLM em partes (streaming), útil para respostas longas ou para exibir a resposta em tempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando stream para receber a resposta em partes\n",
    "for chunk in llm.stream(\"Explique a teoria da relatividade.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. O que são ChatModels?\n",
    "\n",
    "ChatModels são modelos de linguagem projetados para interações em formato de chat, onde a conversa é mantida em um contexto de mensagens (histórico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Inicializando um ChatModel\n",
    "chat = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Criando uma conversa\n",
    "mensagens = [\n",
    "    SystemMessage(content=\"Você é um assistente útil.\"),\n",
    "    HumanMessage(content=\"Quem descobriu a penicilina?\")\n",
    "]\n",
    "\n",
    "# Chamando o ChatModel\n",
    "resposta = chat.invoke(mensagens)\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exemplo Completo: Integrando LLM e ChatModel\n",
    "\n",
    "Vamos criar um exemplo completo que usa tanto LLM quanto ChatModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Inicializando LLM e ChatModel\n",
    "llm = OpenAI(api_key=openai_api_key)\n",
    "chat = ChatOpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Usando LLM com invoke\n",
    "resposta_llm = llm.invoke(\"Explique o que é inteligência artificial.\")\n",
    "print(\"Resposta do LLM:\", resposta_llm)\n",
    "\n",
    "# Usando ChatModel com invoke\n",
    "mensagens = [\n",
    "    SystemMessage(content=\"Você é um assistente especializado em ciência.\"),\n",
    "    HumanMessage(content=\"Quem foi Marie Curie?\")\n",
    "]\n",
    "resposta_chat = chat.invoke(mensagens)\n",
    "print(\"Resposta do ChatModel:\", resposta_chat.content)\n",
    "\n",
    "# Usando stream com LLM\n",
    "print(\"Streaming da resposta do LLM:\")\n",
    "for chunk in llm.stream(\"Descreva o sistema solar.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2: Trabalhando com Prompts no LangChain\n",
    "\n",
    "Nesta segunda aula, vamos explorar o uso de **Prompt Templates** no LangChain. Vamos cobrir:\n",
    "\n",
    "1. **Prompt Templates**: Como criar e usar templates de prompts.\n",
    "2. **Composing Prompts**: Como combinar múltiplos prompts.\n",
    "3. **Templates para Chat**: Como criar templates para interações em formato de chat.\n",
    "4. **Templates de Few-shot para LLMs**: Como usar exemplos para guiar a geração de texto.\n",
    "5. **Templates de Few-shot para Chat**: Como aplicar few-shot learning em modelos de chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. O que são Prompt Templates?\n",
    "\n",
    "Prompt Templates são modelos pré-definidos que ajudam a estruturar a entrada para um LLM. Eles permitem que você crie prompts dinâmicos, reutilizáveis e adaptáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Criando um Prompt Template\n",
    "template = \"Explique o conceito de {conceito} em {nivel} palavras.\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"conceito\", \"nivel\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Preenchendo o template\n",
    "prompt_formatado = prompt.format(conceito=\"inteligência artificial\", nivel=\"50\")\n",
    "print(prompt_formatado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Composing Prompts\n",
    "\n",
    "Você pode combinar múltiplos prompts para criar fluxos de trabalho mais complexos. Isso é útil quando você precisa de várias etapas para gerar uma resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Criando dois prompts\n",
    "template1 = \"Descreva o conceito de {conceito}.\"\n",
    "template2 = \"Agora, explique como {conceito} é aplicado em {area}.\"\n",
    "\n",
    "prompt1 = PromptTemplate(input_variables=[\"conceito\"], template=template1)\n",
    "prompt2 = PromptTemplate(input_variables=[\"conceito\", \"area\"], template=template2)\n",
    "\n",
    "# Combinando os prompts\n",
    "descricao = prompt1.format(conceito=\"machine learning\")\n",
    "aplicacao = prompt2.format(conceito=\"machine learning\", area=\"medicina\")\n",
    "\n",
    "print(descricao)\n",
    "print(aplicacao)\n",
    "\n",
    "template_final = (descricao+aplicacao)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(template_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Templates para Chat\n",
    "\n",
    "Templates para Chat são usados para criar interações em formato de chat, onde o histórico de mensagens é mantido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Criando um template para chat\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Você é um assistente útil.\"),    \n",
    "    (\"human\", \"Explique o que é {conceito}.\")\n",
    "])\n",
    "\n",
    "# Preenchendo o template\n",
    "template_final = template.format_messages(conceito=\"blockchain\")\n",
    "print(template_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Templates de Few-shot para LLMs\n",
    "\n",
    "Few-shot learning envolve fornecer alguns exemplos ao modelo para guiar sua geração de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Exemplos para few-shot learning\n",
    "exemplos = [\n",
    "    {\"pergunta\": \"O que é um átomo?\", \"resposta\": \"Um átomo é a unidade básica da matéria.\"},\n",
    "    {\"pergunta\": \"O que é uma célula?\", \"resposta\": \"Uma célula é a unidade básica da vida.\"}\n",
    "]\n",
    "\n",
    "# Template para cada exemplo\n",
    "exemplo_template = PromptTemplate(\n",
    "    input_variables=[\"pergunta\", \"resposta\"],\n",
    "    template=\"Pergunta: {pergunta}\\nResposta: {resposta}\"\n",
    ")\n",
    "\n",
    "# Criando o Few-shot Prompt Template\n",
    "few_shot_template = FewShotPromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=exemplo_template,\n",
    "    prefix=\"Responda as perguntas seguindo os exemplos:\",\n",
    "    suffix=\"Pergunta: {pergunta}\\nResposta:\",\n",
    "    input_variables=[\"pergunta\"]\n",
    ")\n",
    "\n",
    "# Preenchendo o template\n",
    "prompt = few_shot_template.format(pergunta=\"O que é um gene?\")\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Templates de Few-shot para Chat\n",
    "\n",
    "Few-shot learning também pode ser aplicado em modelos de chat, fornecendo exemplos de interações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Exemplos para few-shot learning\n",
    "exemplos = [\n",
    "    {\"input\": \"O que é um átomo?\", \"output\": \"Um átomo é a unidade básica da matéria.\"},\n",
    "    {\"input\": \"O que é uma célula?\", \"output\": \"Uma célula é a unidade básica da vida.\"}\n",
    "]\n",
    "\n",
    "# Template para cada exemplo\n",
    "exemplo_template = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "# Criando o Few-shot Chat Template\n",
    "few_shot_chat_template = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=exemplo_template,\n",
    "    examples=exemplos\n",
    ")\n",
    "\n",
    "# Combinando com um template de chat\n",
    "final_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente útil.\"),\n",
    "    few_shot_chat_template,\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Preenchendo o template\n",
    "mensagens = final_template.format_messages(input=\"O que é um gene?\")\n",
    "print(mensagens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 3: Trabalhando com Output Parsers no LangChain\n",
    "## 1. O que são Output Parsers?\n",
    "\n",
    "Output Parsers são usados para transformar a saída bruta de um LLM em um formato mais estruturado e útil. Eles são especialmente úteis quando você precisa que a saída do modelo siga um padrão específico, como JSON, listas ou outros formatos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tipos de Output Parsers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 `StructuredOutputParser`\n",
    "\n",
    "O `StructuredOutputParser` é usado para saídas estruturadas, como JSON. Ele permite que você defina um esquema para a saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# Definindo o esquema da resposta\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"conceito\", description=\"O conceito explicado.\"),\n",
    "    ResponseSchema(name=\"explicacao\", description=\"A explicação do conceito.\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Criando um Prompt Template\n",
    "template = \"\"\"\n",
    "Explique o conceito de {conceito} em {nivel} palavras.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"conceito\", \"nivel\"],\n",
    "    template=template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Inicializando o LLM\n",
    "llm = OpenAI()\n",
    "\n",
    "# Criando a cadeia com StructuredOutputParser\n",
    "chain = LLMChain(llm=llm, prompt=prompt, output_parser=output_parser)\n",
    "\n",
    "# Executando a cadeia\n",
    "resposta = chain.run(conceito=\"inteligência artificial\", nivel=\"50\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 4: Trabalhando com SequentialChain no LangChain\n",
    "\n",
    "## 1. O que é SequentialChain?\n",
    "\n",
    "SequentialChain é uma cadeia que permite executar uma sequência de operações, onde a saída de uma etapa é passada como entrada para a próxima. Isso é útil quando você precisa realizar várias tarefas em sequência, como gerar texto, processar informações e tomar decisões com base nos resultados.\n",
    "\n",
    "---\n",
    "## 2. Como criar uma SequentialChain\n",
    "\n",
    "### 2.1 Criando chain's\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1097801/674555536.py:14: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain_texto = LLMChain(llm=llm, prompt=prompt, output_key='texto_pt')\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Inicializando o LLM\n",
    "llm = OpenAI()\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "'''Texto de um idioma aleatório: {texto_idioma}.\n",
    "Retorne texto traduzido em português.'''\n",
    ")\n",
    "\n",
    "chain_texto = LLMChain(llm=llm, prompt=prompt, output_key='texto_pt')\n",
    "\n",
    "prompt_resumo = PromptTemplate.from_template(\n",
    "'''Dado o texto em português: {texto_pt}.\n",
    "Faça um resumo de até 50 palavras desse texto.'''\n",
    ")\n",
    "\n",
    "chain_resumo = LLMChain(llm=llm, prompt=prompt_resumo, output_key='texto_resumo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Criando Sequência ou Encadeamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'texto_idioma': \"Le soleil se lève doucement sur la ville, illuminant les rues d'une lumière dorée. Les oiseaux chantent, annonçant le début d'une nouvelle journée. Les habitants sortent peu à peu de chez eux, prêts à affronter les défis qui les attendent.Dans l'air, on sent une énergie positive, comme si tout était possible. C'est le moment de croire en ses rêves et de saisir les opportunités qui se présentent.\",\n",
       " 'texto_pt': '\\n\\nO sol está se levantando suavemente sobre a cidade, iluminando as ruas com uma luz dourada. Os pássaros cantam, anunciando o início de um novo dia. Os habitantes saem aos poucos de suas casas, prontos para enfrentar os desafios que os aguardam. No ar, sente-se uma energia positiva, como se tudo fosse possível. É hora de acreditar em seus sonhos e aproveitar as oportunidades que surgem.',\n",
       " 'texto_resumo': '\\n\\nO sol nasce sobre a cidade, trazendo uma luz dourada e uma energia positiva. Os pássaros cantam e os habitantes se preparam para enfrentar os desafios do dia. É hora de acreditar nos sonhos e aproveitar as oportunidades que surgem com o novo amanhecer.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = SequentialChain(\n",
    "    chains=[chain_texto, chain_resumo],\n",
    "    input_variables=['texto_idioma'],\n",
    "    output_variables=['texto_pt', 'texto_resumo'],\n",
    "    verbose=True\n",
    ")\n",
    "texto = \"Le soleil se lève doucement sur la ville, illuminant les rues d'une lumière dorée. Les oiseaux chantent, \\\n",
    "annonçant le début d'une nouvelle journée. Les habitants sortent peu à peu de chez eux, prêts à affronter les défis qui les attendent.\\\n",
    "Dans l'air, on sent une énergie positive, comme si tout était possible. C'est le moment de croire en ses rêves et de saisir les opportunités qui se présentent.\"\n",
    "\n",
    "resposta = chain.invoke({'texto_idioma': texto})\n",
    "resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de Encadeamento Simples usando LLM diferentes:\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "# Inicializando os LLMs\n",
    "llm1 = OpenAI(api_key=\"sua_chave_api\", temperature=0.7)\n",
    "llm2 = OpenAI(api_key=\"sua_chave_api\", temperature=0.7)\n",
    "\n",
    "# Criando o primeiro Prompt Template\n",
    "template1 = \"Explique o conceito de {conceito}.\"\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=[\"conceito\"],\n",
    "    template=template1\n",
    ")\n",
    "\n",
    "# Criando o segundo Prompt Template\n",
    "template2 = \"Agora, explique como {conceito} é aplicado em {area}.\"\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"conceito\", \"area\"],\n",
    "    template=template2\n",
    ")\n",
    "\n",
    "# Criando as cadeias\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1, output_key=\"explicacao_conceito\")\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2, output_key=\"aplicacao_conceito\")\n",
    "\n",
    "# Criando a SequentialChain\n",
    "sequential_chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=[\"conceito\", \"area\"],\n",
    "    output_variables=[\"explicacao_conceito\", \"aplicacao_conceito\"]\n",
    ")\n",
    "\n",
    "# Executando a SequentialChain\n",
    "resultado = sequential_chain({\"conceito\": \"inteligência artificial\", \"area\": \"medicina\"})\n",
    "print(\"Explicação do conceito:\", resultado[\"explicacao_conceito\"])\n",
    "print(\"Aplicação do conceito:\", resultado[\"aplicacao_conceito\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Desafio - Chains**\n",
    "Crie as seguintes chains:\n",
    "\n",
    "1) Uma chain para pegar um nome completo de uma pessoa e converter em caixa alta\n",
    "2) Uma chain para criptografar\n",
    "3) Mostre resultado final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
