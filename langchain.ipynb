{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 1: Estrutura Básica do LangChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Passo 1. Instalação das Dependências**\n",
    "   Primeiro, certifique-se de que você tem o LangChain instalado. Você pode instalá-lo usando pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain langchain-community python-dotenv openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Passo 2: Configurar a Chave da API**\n",
    "\n",
    "1. **Obtenha sua chave da API:**\n",
    "\n",
    "   - **OpenAI**: Acesse o [site da OpenAI](https://platform.openai.com/account/api-keys) e gere sua chave da API. Certifique-se de copiá-la e armazená-la em local seguro, pois você não poderá visualizá-la novamente.\n",
    "   - **Gemini**: Acesse o [AI Studio da Google](https://aistudio.google.com/app/apikey), faça login com sua conta Google e siga as instruções para gerar a chave de acesso à API Gemini.\n",
    "   - **DeepSeek**: Vá até o [site oficial da DeepSeek](https://www.deepseek.com/), faça login e gere sua chave da API no painel de configurações.\n",
    "\n",
    "2. **Armazene sua chave em um arquivo `.env`:**\n",
    "\n",
    "   Crie ou edite um arquivo chamado `.env` no diretório do seu projeto e adicione as chaves obtidas, seguindo o formato abaixo:\n",
    "\n",
    "   ```plaintext\n",
    "   OPENAI_API_KEY=your_openai_api_key_here\n",
    "   ```\n",
    "\n",
    "3. **Carregue as chaves no seu código:**\n",
    "\n",
    "   Use uma biblioteca como `python-dotenv` para carregar as variáveis de ambiente no seu programa:\n",
    "\n",
    "   ```python\n",
    "   from dotenv import load_dotenv\n",
    "   import os\n",
    "\n",
    "   # Carregar as variáveis do arquivo .env\n",
    "   load_dotenv()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Passo 3: Chamando um LLM com `invoke`**\n",
    "\n",
    "\n",
    "O método `invoke` é usado para enviar uma entrada ao LLM e receber uma resposta completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Carregar as variáveis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obter as chaves das APIs\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# Certifique-se de que as chaves foram carregadas\n",
    "if not all([openai_api_key, gemini_api_key, deepseek_api_key]):\n",
    "    raise ValueError(\"Uma ou mais chaves de API não foram configuradas corretamente.\")\n",
    "\n",
    "# Inicializando um LLM \n",
    "llm = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Chamando o LLM com uma entrada\n",
    "resposta = llm.invoke(\"Explique o que é um LLM.\")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Passo 3.1: Chamando um LLM com `invoke` com Gemini**\n",
    "\n",
    "\n",
    "O método `invoke` é usado para enviar uma entrada ao LLM e receber uma resposta completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Set cache to save results to memory\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# Carregar as variáveis do arquivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obter as chaves das APIs\n",
    "gemini_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "# Correctly initialize the ChatGoogleGenerativeAI model\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=gemini_api_key, top_p=0.30)\n",
    "\n",
    "# Prepare the input messages\n",
    "input_messages = [\n",
    "    HumanMessage(content=\"Quem é o presidente do Brasil?\")\n",
    "]\n",
    "\n",
    "# Use the model\n",
    "response = llm.invoke(input_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Passo 3.2: Chamando um LLM com `invoke` com Deepseek**\n",
    "\n",
    "\n",
    "O método `invoke` é usado para enviar uma entrada ao LLM e receber uma resposta completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Carrega a chave do .env\n",
    "\n",
    "# Configuração do DeepSeek\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    model=\"deepseek-chat\",  # ou \"deepseek-reasoner\"\n",
    "    openai_api_base=\"https://api.deepseek.com/v1\",  # Endpoint da API\n",
    "     \n",
    ")\n",
    "\n",
    "# Exemplo de uso\n",
    "response = llm.invoke(\"Quem é o presidente do Brasil?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stream de Respostas com `stream`\n",
    "\n",
    "O método `stream` permite receber a resposta do LLM em partes (streaming), útil para respostas longas ou para exibir a resposta em tempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando stream para receber a resposta em partes\n",
    "for chunk in llm.stream(\"Explique a teoria da relatividade.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. O que são ChatModels?\n",
    "\n",
    "ChatModels são modelos de linguagem projetados para interações em formato de chat, onde a conversa é mantida em um contexto de mensagens (histórico)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Inicializando um ChatModel\n",
    "chat = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Criando uma conversa\n",
    "mensagens = [\n",
    "    SystemMessage(content=\"Você é um assistente útil.\"),\n",
    "    HumanMessage(content=\"Quem descobriu a penicilina?\")\n",
    "]\n",
    "\n",
    "# Chamando o ChatModel\n",
    "resposta = chat.invoke(mensagens)\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exemplo Completo: Integrando LLM e ChatModel\n",
    "\n",
    "Vamos criar um exemplo completo que usa tanto LLM quanto ChatModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Inicializando LLM e ChatModel\n",
    "llm = OpenAI(api_key=openai_api_key)\n",
    "chat = ChatOpenAI(api_key=openai_api_key)\n",
    "\n",
    "# Usando LLM com invoke\n",
    "resposta_llm = llm.invoke(\"Explique o que é inteligência artificial.\")\n",
    "print(\"Resposta do LLM:\", resposta_llm)\n",
    "\n",
    "# Usando ChatModel com invoke\n",
    "mensagens = [\n",
    "    SystemMessage(content=\"Você é um assistente especializado em ciência.\"),\n",
    "    HumanMessage(content=\"Quem foi Marie Curie?\")\n",
    "]\n",
    "resposta_chat = chat.invoke(mensagens)\n",
    "print(\"Resposta do ChatModel:\", resposta_chat.content)\n",
    "\n",
    "# Usando stream com LLM\n",
    "print(\"Streaming da resposta do LLM:\")\n",
    "for chunk in llm.stream(\"Descreva o sistema solar.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 2: Trabalhando com Prompts no LangChain\n",
    "\n",
    "Nesta segunda aula, vamos explorar o uso de **Prompt Templates** no LangChain. Vamos cobrir:\n",
    "\n",
    "1. **Prompt Templates**: Como criar e usar templates de prompts.\n",
    "2. **Composing Prompts**: Como combinar múltiplos prompts.\n",
    "3. **Templates para Chat**: Como criar templates para interações em formato de chat.\n",
    "4. **Templates de Few-shot para LLMs**: Como usar exemplos para guiar a geração de texto.\n",
    "5. **Templates de Few-shot para Chat**: Como aplicar few-shot learning em modelos de chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. O que são Prompt Templates?\n",
    "\n",
    "Prompt Templates são modelos pré-definidos que ajudam a estruturar a entrada para um LLM. Eles permitem que você crie prompts dinâmicos, reutilizáveis e adaptáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Criando um Prompt Template\n",
    "template = \"Explique o conceito de {conceito} em {nivel} palavras.\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"conceito\", \"nivel\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Preenchendo o template\n",
    "prompt_formatado = prompt.format(conceito=\"inteligência artificial\", nivel=\"50\")\n",
    "print(prompt_formatado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Composing Prompts\n",
    "\n",
    "Você pode combinar múltiplos prompts para criar fluxos de trabalho mais complexos. Isso é útil quando você precisa de várias etapas para gerar uma resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Criando dois prompts\n",
    "template1 = \"Descreva o conceito de {conceito}.\"\n",
    "template2 = \"Agora, explique como {conceito} é aplicado em {area}.\"\n",
    "\n",
    "prompt1 = PromptTemplate(input_variables=[\"conceito\"], template=template1)\n",
    "prompt2 = PromptTemplate(input_variables=[\"conceito\", \"area\"], template=template2)\n",
    "\n",
    "# Combinando os prompts\n",
    "descricao = prompt1.format(conceito=\"machine learning\")\n",
    "aplicacao = prompt2.format(conceito=\"machine learning\", area=\"medicina\")\n",
    "\n",
    "print(descricao)\n",
    "print(aplicacao)\n",
    "\n",
    "template_final = (descricao+aplicacao)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(template_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Templates para Chat\n",
    "\n",
    "Templates para Chat são usados para criar interações em formato de chat, onde o histórico de mensagens é mantido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# Criando um template para chat\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"Você é um assistente útil.\"),    \n",
    "    (\"human\", \"Explique o que é {conceito}.\")\n",
    "])\n",
    "\n",
    "# Preenchendo o template\n",
    "template_final = template.format_messages(conceito=\"blockchain\")\n",
    "print(template_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Templates de Few-shot para LLMs\n",
    "\n",
    "Few-shot learning envolve fornecer alguns exemplos ao modelo para guiar sua geração de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "# Exemplos para few-shot learning\n",
    "exemplos = [\n",
    "    {\"pergunta\": \"O que é um átomo?\", \"resposta\": \"Um átomo é a unidade básica da matéria.\"},\n",
    "    {\"pergunta\": \"O que é uma célula?\", \"resposta\": \"Uma célula é a unidade básica da vida.\"}\n",
    "]\n",
    "\n",
    "# Template para cada exemplo\n",
    "exemplo_template = PromptTemplate(\n",
    "    input_variables=[\"pergunta\", \"resposta\"],\n",
    "    template=\"Pergunta: {pergunta}\\nResposta: {resposta}\"\n",
    ")\n",
    "\n",
    "# Criando o Few-shot Prompt Template\n",
    "few_shot_template = FewShotPromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=exemplo_template,\n",
    "    prefix=\"Responda as perguntas seguindo os exemplos:\",\n",
    "    suffix=\"Pergunta: {pergunta}\\nResposta:\",\n",
    "    input_variables=[\"pergunta\"]\n",
    ")\n",
    "\n",
    "# Preenchendo o template\n",
    "prompt = few_shot_template.format(pergunta=\"O que é um gene?\")\n",
    "print(prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Templates de Few-shot para Chat\n",
    "\n",
    "Few-shot learning também pode ser aplicado em modelos de chat, fornecendo exemplos de interações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# Exemplos para few-shot learning\n",
    "exemplos = [\n",
    "    {\"input\": \"O que é um átomo?\", \"output\": \"Um átomo é a unidade básica da matéria.\"},\n",
    "    {\"input\": \"O que é uma célula?\", \"output\": \"Uma célula é a unidade básica da vida.\"}\n",
    "]\n",
    "\n",
    "# Template para cada exemplo\n",
    "exemplo_template = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\")\n",
    "])\n",
    "\n",
    "# Criando o Few-shot Chat Template\n",
    "few_shot_chat_template = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=exemplo_template,\n",
    "    examples=exemplos\n",
    ")\n",
    "\n",
    "# Combinando com um template de chat\n",
    "final_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente útil.\"),\n",
    "    few_shot_chat_template,\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Preenchendo o template\n",
    "mensagens = final_template.format_messages(input=\"O que é um gene?\")\n",
    "print(mensagens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 3: Trabalhando com Output Parsers no LangChain\n",
    "## 1. O que são Output Parsers?\n",
    "\n",
    "Output Parsers são usados para transformar a saída bruta de um LLM em um formato mais estruturado e útil. Eles são especialmente úteis quando você precisa que a saída do modelo siga um padrão específico, como JSON, listas ou outros formatos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tipos de Output Parsers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 `StructuredOutputParser`\n",
    "\n",
    "O `StructuredOutputParser` é usado para saídas estruturadas, como JSON. Ele permite que você defina um esquema para a saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conceito': 'Inteligência Artificial', 'explicacao': 'É um campo da ciência da computação que busca desenvolver máquinas e sistemas que possam realizar tarefas que, normalmente, exigem inteligência humana. Isso é feito através de algoritmos e técnicas que permitem que as máquinas aprendam, raciocinem, tomem decisões e se adaptem a novas situações.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# Definindo o esquema da resposta\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"conceito\", description=\"O conceito explicado.\"),\n",
    "    ResponseSchema(name=\"explicacao\", description=\"A explicação do conceito.\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Criando um Prompt Template\n",
    "template = \"\"\"\n",
    "Explique o conceito de {conceito} em {nivel} palavras.\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"conceito\", \"nivel\"],\n",
    "    template=template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "# Inicializando o LLM\n",
    "llm = OpenAI()\n",
    "\n",
    "# Criando a cadeia com StructuredOutputParser\n",
    "chain = LLMChain(llm=llm, prompt=prompt, output_parser=output_parser)\n",
    "\n",
    "# Executando a cadeia\n",
    "resposta = chain.run(conceito=\"inteligência artificial\", nivel=\"50\")\n",
    "print(resposta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
